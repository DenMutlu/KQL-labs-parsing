{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p># Welcome to the Parsing &amp; Advanced KQL training session</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the Advanced Security Information Model (ASIM) Training! </p> <p>In today\u2019s session, we\u2019ll explore the Parsing fundamentals, the Microsoft Sentinel\u2019s ASIM framework and how it can be used to normalize and standardize data across various sources. Parsing data with ASIM allows for more efficient threat detection, cross-source analytics, and streamlined data correlation, empowering security analysts to tackle complex challenges with consistency and ease. Finally we will practice on Azure Data Explorer with Advanced KQL Queries!</p> <p>During this training, we\u2019ll cover: - An overview of ASIM, its components, and its place within Microsoft Sentinel. - Hands-on exercises to practice using ASIM parsers and schemas for querying data. - How to create custom rules, analytics, and workbooks that leverage ASIM's power.</p> <p>By the end of this session, you\u2019ll have a solid foundation in using ASIM to enhance your data\u2019s consistency and make your security operations more effective.</p>"},{"location":"#good-luck","title":"Good Luck!","text":"<p>Learning new tools and concepts can sometimes be challenging, but it\u2019s also rewarding. Stay curious, ask questions, and, most importantly, enjoy the learning process. This is an opportunity to deepen your skills and contribute to a stronger security posture within your organization.</p>"},{"location":"#lets-get-started-good-luck-and-happy-learning","title":"Let's get started \u2014 good luck and happy learning!","text":""},{"location":"#den","title":"Den","text":""},{"location":"detectives1/","title":"Kusto Detective Agency Season 1","text":"<p>Now you solve the two first challenge, let's continue with the Official platform ;)</p> <p>Kusto Detective Agency </p> <p>The Goal is to win the credly badges</p>"},{"location":"lab1-solution/","title":"Lab 1 : Solution","text":""},{"location":"lab1-solution/#explanation-in-english-french-below","title":"Explanation in English (french below)","text":""},{"location":"lab1-solution/#explanation-in-french","title":"Explanation in French","text":""},{"location":"lab1-solution/#explication-de-lexpression-reguliere-bproxysd","title":"Explication de l'expression r\u00e9guli\u00e8re : <code>(?&lt;=\\bproxy\\s)\\d+</code>","text":"<p>This regular expression is designed to capture a number that appears immediately after the word \"proxy \" (with a space after it). Here is a detailed explanation of each component of the expression:</p>"},{"location":"lab1-solution/#expression-details","title":"Expression Details","text":"<ul> <li><code>(?&lt;= ... )</code> - Positive Lookbehind Assertion</li> </ul> <p><code>(?&lt;= ... )</code> is a positive lookbehind assertion. A lookbehind is a type of assertion that checks if a certain pattern appears before the current position in the text string. An important aspect of lookbehind is that it does not include the text it checks in the final match result.</p> <p>In this case, <code>(?&lt;=\\bproxy\\s)</code> ensures that \"proxy\" followed by a space appears immediately before the position where the capture begins.</p> <ul> <li><code>\\b</code> - Word Boundary</li> </ul> <p><code>\\b</code> is a word boundary, which matches the position between a word character (e.g., <code>[A-Za-z0-9_]</code>) and a non-word character. Here, <code>\\b</code> before \"proxy\" ensures that \"proxy\" is matched as a whole word. For example, it would not match \"xproxy\" or \"proxying\" because these words do not fully match \"proxy\".</p> <ul> <li><code>proxy</code> - Literal Text</li> </ul> <p>This part of the expression matches the exact word \"proxy\". Combined with <code>\\b</code> and <code>\\s</code>, it ensures that only numbers appearing after a complete match of the word \"proxy \" (with a space after it) are captured.</p> <ul> <li><code>\\s</code> - Whitespace Character</li> </ul> <p><code>\\s</code> matches any whitespace character (such as a space, tab, etc.). Here, it ensures that there is at least one space after \"proxy\" before the number begins.</p> <ul> <li><code>\\d+</code> - Digit(s)</li> </ul> <p><code>\\d</code> matches a single digit (0-9). The quantifier <code>+</code> means \"one or more\" of the preceding character (in this case, <code>\\d</code>). Together, <code>\\d+</code> matches one or more consecutive digits, representing the number (such as a port number) that follows \"proxy \".</p>"},{"location":"lab1-solution/#how-the-complete-expression-works","title":"How the Complete Expression Works","text":"<p>Complete Expression: <code>(?&lt;=\\bproxy\\s)\\d+</code></p> <ol> <li>The lookbehind <code>(?&lt;=\\bproxy\\s)</code> ensures that the expression finds a sequence of digits following the word \"proxy\" as an isolated word, immediately followed by a space.</li> <li>The <code>\\d+</code> part captures the number (one or more digits) that appears after \"proxy \".</li> </ol>"},{"location":"lab1-solution/#usage-example","title":"Usage Example","text":"<p>To illustrate, if we apply this expression to the following string:</p> <pre><code>proxy 8080\n</code></pre> <p>The pattern <code>(?&lt;=\\bproxy\\s)\\d+</code> will capture <code>8080</code> because:</p> <ul> <li>\"proxy\" is followed by a space,</li> <li><code>8080</code> is a set of consecutive digits that appears right after \"proxy \".</li> </ul> <p></p> <p>Cette expression r\u00e9guli\u00e8re est con\u00e7ue pour capturer un nombre qui appara\u00eet imm\u00e9diatement apr\u00e8s le mot \"proxy \" (avec un espace apr\u00e8s). Voici une explication d\u00e9taill\u00e9e de chaque composant de l'expression :</p>"},{"location":"lab1-solution/#details-de-lexpression","title":"D\u00e9tails de l'expression","text":"<ul> <li><code>(?&lt;= ... )</code> - Assertion de Lookbehind positif</li> </ul> <p><code>(?&lt;= ... )</code> est une assertion de lookbehind positif. Un lookbehind est un type d'assertion qui v\u00e9rifie si un certain motif appara\u00eet avant la position actuelle dans la cha\u00eene de caract\u00e8res. Ce qui est important avec un lookbehind, c'est qu'il ne capture pas le texte qu'il v\u00e9rifie dans le r\u00e9sultat final de la correspondance.</p> <p>Dans ce cas, <code>(?&lt;=\\bproxy\\s)</code> v\u00e9rifie que \"proxy\" suivi d'un espace appara\u00eet imm\u00e9diatement avant la position o\u00f9 l'on souhaite commencer la capture.</p> <ul> <li><code>\\b</code> - D\u00e9limiteur de mot (Fronti\u00e8re)</li> </ul> <p><code>\\b</code> est un d\u00e9limiteur de mot, qui correspond \u00e0 la position entre un caract\u00e8re de mot (par exemple, <code>[A-Za-z0-9_]</code>) et un caract\u00e8re non-mot. Ici, <code>\\b</code> avant \"proxy\" s'assure que \"proxy\" est captur\u00e9 comme un mot complet. Par exemple, cela ne correspondra pas \u00e0 \"xproxy\" ou \"proxying\", car ces mots ne correspondent pas enti\u00e8rement \u00e0 \"proxy\".</p> <ul> <li><code>proxy</code> - Texte litt\u00e9ral</li> </ul> <p>Cette partie de l'expression correspond exactement au mot \"proxy\". Combin\u00e9 avec <code>\\b</code> et <code>\\s</code>, cela garantit que seuls les nombres situ\u00e9s apr\u00e8s une correspondance compl\u00e8te du mot \"proxy \" (avec un espace apr\u00e8s) sont captur\u00e9s.</p> <ul> <li><code>\\s</code> - Caract\u00e8re d'espace</li> </ul> <p><code>\\s</code> correspond \u00e0 n'importe quel caract\u00e8re d'espace (comme un espace, une tabulation, etc.). Ici, il est utilis\u00e9 pour s'assurer qu'il y a au moins un espace apr\u00e8s \"proxy\" avant que le nombre commence.</p> <ul> <li><code>\\d+</code> - Chiffre(s)</li> </ul> <p><code>\\d</code> correspond \u00e0 un chiffre unique (0-9). Le quantificateur <code>+</code> signifie \"un ou plusieurs\" du caract\u00e8re pr\u00e9c\u00e9dent (ici, <code>\\d</code>). Ensemble, <code>\\d+</code> capture un ou plusieurs chiffres cons\u00e9cutifs, repr\u00e9sentant le nombre (comme un num\u00e9ro de port) qui suit \"proxy \".</p>"},{"location":"lab1-solution/#comment-fonctionne-lexpression-complete","title":"Comment fonctionne l\u2019expression compl\u00e8te","text":"<p>Expression compl\u00e8te : <code>(?&lt;=\\bproxy\\s)\\d+</code></p> <ol> <li>Le lookbehind <code>(?&lt;=\\bproxy\\s)</code> s'assure que l'expression trouve une s\u00e9quence de chiffres qui suit le mot \"proxy\" en tant que mot isol\u00e9, suivi imm\u00e9diatement par un espace.</li> <li>La partie <code>\\d+</code> capture le nombre (un ou plusieurs chiffres) qui vient apr\u00e8s \"proxy \".</li> </ol>"},{"location":"lab1-solution/#exemple-dutilisation","title":"Exemple d'utilisation","text":"<p>Pour illustrer, si nous appliquons cette expression \u00e0 la cha\u00eene suivante :</p> <pre><code>proxy 8080\n</code></pre> <p>Le motif <code>plaintext (?&lt;=\\bproxy\\s)\\d+</code> capturera <code>plaintext 8080</code> parce que :</p> <ul> <li>\"proxy\" est suivi d'un espace,</li> <li><code>8080</code> est un ensemble de chiffres cons\u00e9cutifs, juste apr\u00e8s \"proxy \".</li> </ul>"},{"location":"lab1-solution/#explanation-in-english","title":"Explanation in English","text":""},{"location":"lab1/","title":"Lab 1 : Regex (20 min)","text":"<p>Here is a log file <pre><code>0.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 8080 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:18 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 0\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 0\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 8800 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 2\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 8080 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 4\n2024-09-03 10:14:19 10.0.2.15 GET /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 8\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:19 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:20 10.0.2.15 GET /erp/conn.aspx proxy 8070 - 10.0.2.17 - - 200 0 0 2\n2024-09-03 10:14:20 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n2024-09-03 10:14:20 10.0.2.15 POST /erp/conn.aspx proxy 80 - 10.0.2.17 - - 200 0 0 1\n</code></pre></p> <p>Your goal is to have a field \"port\" in your favourite SIEM that show only the port number. You need to extract the port from the log using Regular Expression</p> <ul> <li>1) Use the regex101 website to craft your regex</li> <li>2) Insert a simple of the log into the \"test strings\" area </li> <li>3) Craft your query (without IA for the moment), if you are novice using regex regex ressources</li> <li>4) Test your query, step by step into the \"regular expression\" area to see if you catch is good or not</li> <li>5) If you struggle and you try at least 10 min to craft your query alone, use your favourite IA platform to craft the query </li> </ul>"},{"location":"lab2-1/","title":"Lab 2 : Advanced KQL - Challenge 1","text":""},{"location":"lab2-1/#the-rarest-book-is-missing","title":"The rarest book is missing!","text":"<p>This was supposed to be a great day for Digitown\u2019s National Library Museum and all of Digitown.</p> <p>The museum has just finished scanning over 325,000 rare books so that history lovers worldwide can experience the ancient culture and knowledge of the Digitown Explorers. The grand book exhibition was about to re-open when the museum director noticed something alarming: the rarest book in the world was missing!</p> <p>The missing artifact is titled \"De Revolutionibus Magnis Data\", published in 1613 by Gustav Kustov.</p> <p>The mayor of Digitown, Mrs. Gaia Budskott, has personally requested our agency to help locate this precious book.</p>"},{"location":"lab2-1/#key-details-about-the-digitown-library-system","title":"Key Details About the Digitown Library System","text":"<p>Luckily, everything is digital in the Digitown library:</p> <ul> <li>Each book has its parameters recorded, such as number of pages and weight.</li> <li>Each book is tagged with an RFID sticker (RFID: radio transmitter with a unique ID).</li> <li>Each shelf in the museum is equipped to:</li> <li>Track the RFIDs of the books placed on it.</li> <li>Measure the total weight of books on the shelf.</li> </ul>"},{"location":"lab2-1/#the-challenge","title":"The Challenge","text":"<p>Unfortunately, the RFID tag for \"De Revolutionibus Magnis Data\" was found detached and abandoned on the museum floor. Could you help locate the book on one of the museum shelves and save the day?</p> <p>Add the following datas to your cluster</p> <p><pre><code>.execute database script &lt;|\n// Create table for the books\n.create-merge table Books(rf_id:string, book_title:string, publish_date:long, author:string, language:string, number_of_pages:long, weight_gram:long)\n//clear any previously ingested data if such exists\n.clear table Books data\n// Import data for books\n// (Used data is utilizing catalogue from https://github.com/internetarchive/openlibrary )\n.ingest into table Books ('https://kustodetectiveagency.blob.core.windows.net/digitown-books/books.csv.gz') with (ignoreFirstRecord=true)\n// Create table for the shelves\n.create-merge table Shelves (shelf:long, rf_ids:dynamic, total_weight:long) \n.clear table Shelves data\n// Import data for shelves\n.ingest into table Shelves ('https://kustodetectiveagency.blob.core.windows.net/digitown-books/shelves.csv.gz') with (ignoreFirstRecord=true)\n</code></pre> </p>"},{"location":"lab2-1/#which-shelf-is-the-book-on","title":"Which shelf is the book on?","text":""},{"location":"lab2-1solution/","title":"Lab 2 : Advanced KQL - Challenge 1 (Solution)","text":""},{"location":"lab2-1solution/#key-takeaways-from-the-riddle","title":"Key takeaways from the Riddle","text":"<ul> <li>Each book's weight is known.</li> <li>Each book, except for the missing one, has an RFID badge.</li> <li>Each shelf provides:</li> <li>The actual total weight of books on it.</li> <li>The RFIDs of the books it holds.</li> </ul>"},{"location":"lab2-1solution/#details-of-the-missing-book","title":"Details of the missing book","text":"<ul> <li>Title: De Revolutionibus Magnis Data</li> <li>Author: Gustav Kustov</li> <li> <p>Published: 1613</p> </li> <li> <p>OK, first step is always to see the tables content and what data we can use,</p> </li> <li>Lets check the Books table and the shelves (take time to check all fields at disposition)</li> </ul> <p></p> <ul> <li>when we delve into the data, to just check the fields it could take lot of time to load regarding the number of events, good practice is to use the <code>kql | take 1</code>, to see a single row.</li> </ul> <p></p> <p>The missing book is located on a shelf, so the actual weight of the self is higher than the sum of weights that are reported on that shelf because the book is missing the RFID.</p> <p>The <code>Books</code> table has the following fields: </p> <ul> <li>RFID, Book Title, Publish Data, Author, Language, Number of Pages, and Weight Gram.</li> </ul> <p>The <code>Shelves</code> table has: id, </p> <ul> <li>RFIDs of the books on it, and the total weight.</li> </ul> <p>For each shelf, we\u2019d like to calculate the expected total weight of the books on the shelf, so we can compare it with the reported total weight.</p> <p>Altogether, we are given the RFIDs on the shelf as an array: if only we had them separately, we could have used the RFID of each book to find its weight from the books table, and then sum by shelf.</p> <p>Oh now how to do this? Lucky we are with KQL, the mv-expand operator that lets us to split the RFIDs array of the shelf:</p> <pre><code>Shelves\n| mv-expand rf_ids to typeof(string)\n</code></pre> <p>This results in a row for each book RFID and each shelf:</p> <p></p> <p>Now we\u2019d like to sum of weights of the books on the self, so let\u2019s join the <code>Shelves</code> table with the <code>Books</code> table, then we can utilize the summarize operator to do this aggregative calculation of sum of weights of books by shelf.</p> <p>We\u2019re also interested in the reported weight of the shelf, we can take_any of the weights of the duplicated (for each book) shelf rows.</p> <p>Lastly, we\u2019d like to find the difference in weights of the shelves between :</p> <p>The actual measured weight on the shelf The reported (according to books with RFIDs on the shelf) Keeping in mind the missing book has no RFID tag, so the actual weight is more than reported by the missing book weight:</p> <p>Ok let's try to craft a query to do this!</p>"},{"location":"lab2-1solution/#kql-query-explanation","title":"KQL Query Explanation","text":"<p>This KQL query aims to locate the missing book titled \"De Revolutionibus Magnis Data\" by comparing shelf weights with known book weights.</p>"},{"location":"lab2-1solution/#step-by-step-breakdown","title":"Step-by-Step Breakdown","text":"<p>step 1 Identify the Weight of the Missing Book:</p> <pre><code>   let x = Books\n   | where book_title == \"De Revolutionibus Magnis Data\"\n   | project weight_gram;\n</code></pre> <ul> <li>This segment of the query retrieves the weight of the missing book, De Revolutionibus Magnis Data, from the <code>Books</code> dataset.</li> <li>The missing book\u2019s weight is determined to be 1764 grams.</li> </ul> <p>step 2 Expand Shelf Data to Associate Each RFID with a Shelf:</p> <pre><code>   let id_shelf = Shelves\n| mv-expand rf_ids\n| project shelf, rf_id=tostring(rf_ids);\n</code></pre> <ul> <li>This segment uses <code>mv-expand</code> to split the list of RFIDs associated with each shelf in the <code>Shelves</code> dataset.</li> <li>It creates a table (<code>id_shelf</code>) that maps each RFID to its corresponding shelf.</li> </ul> <p>step 3 Calculate the Total Weight of Books on Each Shelf Based on Known RFIDs:</p> <pre><code>  let existing_shelf = Books\n| join kind=innerunique id_shelf on $left.rf_id==$right.rf_id\n| summarize calculated_weight_gram=sum(weight_gram) by shelf;\n</code></pre> <ul> <li>This part joins the <code>Books</code> dataset with the <code>id_shelf</code> table on RFIDs to calculate the total weight of books for each shelf based on known RFID-tagged books.</li> <li>The result, <code>existing_shelf</code>, contains each shelf with its calculated total book weight.</li> </ul> <p>step 4 Identify the Shelf with Missing Book Weight Discrepancy:</p> <pre><code>  existing_shelf\n| join kind=innerunique Shelves on $left.shelf==$right.shelf\n| where total_weight - calculated_weight_gram &gt;= 1764\n</code></pre> <ul> <li>This final part joins the <code>existing_shelf</code> results with the original <code>Shelves</code> dataset on shelf IDs.</li> <li>It filters to find shelves where the difference between the recorded total weight (<code>total_weight</code>) and the calculated weight of known books (<code>calculated_weight_gram</code>) is greater than or equal to 1764 grams (the weight of the missing book).</li> <li>The resulting shelf is likely the location of the missing book, as it has the appropriate weight discrepancy.</li> </ul> <p>Complete query</p> <pre><code>   let x = Books\n   | where book_title == \"De Revolutionibus Magnis Data\"\n   | project weight_gram;\n   // weight_gram = 1764\n   let id_shelf = Shelves\n   | mv-expand rf_ids\n   | project shelf, rf_id=tostring(rf_ids);\n   let existing_shelf = Books\n   | join kind=innerunique id_shelf on $left.rf_id==$right.rf_id\n   | summarize calculated_weight_gram=sum(weight_gram) by shelf;\n   existing_shelf\n   | join kind=innerunique Shelves on $left.shelf==$right.shelf\n   | where total_weight - calculated_weight_gram &gt;= 1764\n</code></pre>"},{"location":"lab2-2/","title":"Lab 2 : Advanced KQL - Challenge 2","text":""},{"location":"lab2-2/#election-fraud","title":"Election fraud?","text":"<p>the mayor of Digitown, Mrs. Gaia Budskott, has found herself in quite a pickle. The election for the city\u2019s mascot was run online for the first time, and it was a huge success! Or was it??</p> <p>Over 5 million people voted. Four candidates made it to the final round:</p> <ul> <li>Kastor the Elephant \u2013 The darling of Digitown Zoo</li> <li>Gaul the Octopus \u2013 A Digitown celebrity, who was a whiz at predicting who\u2019d win the local soccer games</li> <li>William (Willie) the Tortoise \u2013 Digitown\u2019s oldest living creature (estimated age - 176.4 years)</li> <li>Poppy the Goldfish \u2013 ex-Mayor Jason Guvid\u2019s childhood pet</li> </ul> <p>The polls predicted a close battle between Kastor and Gaul, but the actual results showed that the ex-mayor\u2019s fish got a whopping 51.7% of all votes! That sure does sound fishy... The mayor is afraid of a vote-tampering scandal that could affect all elections in Digitown! You\u2019ve helped her out last time, and she\u2019s counting on you to get to the bottom of this mystery.</p> <p>If voting fraud happened \u2013 prove it and correct the election numbers: what percentage of the votes did each candidate get?</p> <p>You have access to the elections data: IP, anonymized id, vote, date-time - and the function used for counting the votes.</p> <p>Good luck, rookie. We\u2019re counting on you.</p> <pre><code>.execute database script &lt;|\n// Ingestion may take ~40sec to complete, total 5M+ records\n.create-merge table Votes (Timestamp:datetime, vote:string, via_ip:string, voter_hash_id:string)\n//clear any previously ingested data if such exists\n.clear table Votes data\n.ingest async into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_1.csv.gz')\n.ingest async into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_2.csv.gz')\n.ingest into table Votes (@'https://kustodetectiveagency.blob.core.windows.net/digitown-votes/votes_3.csv.gz')\n</code></pre> <pre><code>// Query that counts the votes:\nVotes\n| summarize Count=count() by vote\n| as hint.materialized=true T\n| extend Total = toscalar(T | summarize sum(Count))\n| project vote, Percentage = round(Count*100.0 / Total, 1), Count\n| order by Count\n</code></pre> <p>What percentage of the votes did each candidate get?</p> <p>Kastor ??% Gaul ??% Willie ??% Poppy ??%</p>"},{"location":"lab2/","title":"Lab 2 : Advanced KQL - Setup","text":""},{"location":"lab2/#setup-the-environment-for-the-kusto-detective-agency","title":"Setup the environment for the Kusto Detective Agency","text":"<p>Your first task is to create a free Kusto cluster which will be your primary investigation tool. Then you\u2019ll have to answer a simple question to get started.</p> <ul> <li> <p>To create a free Kusto cluster, you need either a Microsoft account (MSA) or an Azure Active Directory (AAD) identity. Create your cluster here: https://aka.ms/kustofree</p> </li> <li> <p>Setup your database like the image below and signin with a Microsoft Account</p> </li> </ul> <p>![kql1].(\\assets\\kql3.png){ align=center }</p> <ul> <li>Copy the Cluster URI, as highlighted in the following image:</li> </ul> <p></p> <ul> <li>Next, head over to the query editor. You can either click on the Query button highlighted in the below image, or open in the Kusto Explorer desktop tool.</li> </ul> <p></p> <p>Now, paste the following KQL script into the query editor, and then run it. Make sure you wait around 10 seconds for the script to complete. Don\u2019t run it more than once! This script creates a new table and loads a file located in Azure blob storage. </p> <p><pre><code>.execute database script &lt;|\n// Create table for the data\n.create-merge table Onboarding(Score:long)\n//clear any previously ingested data if such exists\n.clear table Onboarding data\n// Import data\n.ingest into table Onboarding ('https://kustodetectiveagency.blob.core.windows.net/onboarding/onboarding.csv.gz') with (ignoreFirstRecord=true)\n</code></pre> </p> <p>Now, to test that it\u2019s working, sum up the values in the \u201cScore\u201d column</p> <p>What's the sum of the \"Score\" column?</p>"},{"location":"norm/","title":"Querying normalized data","text":"<p>With normalized data, you can run queries across sources using the ASIM schema. For instance, you might want to analyze successful login attempts across all sources:</p> <pre><code>_Im_Authentication(starttime=ago(1d), LogonResult='Success')\n| summarize count() by ActorUserName, bin(TimeGenerated, 1h)\n| order by count_ desc\n</code></pre> <p>This query outputs the top usernames with successful logins in the last day, regardless of the data source.</p>"},{"location":"norm/#summary","title":"Summary","text":"<ul> <li>Parsing transforms raw data into a structured format, making it easier to analyze.</li> <li>Normalization standardizes data fields across sources, enabling consistent queries.</li> <li>ASIM in Microsoft Sentinel provides schemas and parsers to simplify the process of working with diverse data sources.</li> </ul> <p>The final goal of parsing and normalization, is tp query and analyze security data, efficiently across your environment.</p>"},{"location":"parsing/","title":"Parsing Basics and Data Normalization","text":""},{"location":"parsing/#introduction","title":"Introduction","text":"<p>In security operations, data often comes from various sources, each with its unique format and field names. Parsing and data normalization are essential for transforming this data into a consistent, specially for SOC operations, standardized format, making it easier to analyze and correlate information across different sources is one of the most important tasks to have valuable and efficient detection use cases.</p>"},{"location":"parsing/#what-is-parsing","title":"What is Parsing?","text":"<p>Parsing is the process of breaking down raw data into a structured format so it can be analyzed or transformed. ie. In Microsoft Sentinel, parsing often involves using Kusto Query Language (KQL) functions to interpret and organize data from logs and other sources. in Splunk we will use the Common Information Model (CIM) to normalize and directly apply the configuration in the .conf files (props.conf and transform.conf)</p>"},{"location":"parsing/#example-of-parsing-a-firewall-log","title":"Example of Parsing a Firewall Log","text":"<p>Suppose you have a raw log entry from a firewall like this:</p> <pre><code>source_ip=192.168.1.10, dest_ip=10.1.1.5, action=ALLOW, protocol=TCP\n</code></pre> <p>A parsing function can transform this data into a structured format: <pre><code>{\n  \"SourceIP\": \"192.168.1.10\",\n  \"DestinationIP\": \"10.1.1.5\",\n  \"Action\": \"Allow\",\n  \"Protocol\": \"TCP\"\n}\n</code></pre></p> <p>This structure makes the data easier to work with and understand, allowing for consistent querying across sources.</p>"},{"location":"parsing/#what-is-data-normalization","title":"What is Data Normalization?","text":"<p>Data normalization is the process of standardizing data across different sources by mapping fields and values to a common format or schema. This enables analysts to query and analyze data in a consistent way, regardless of the original data source.</p> <p>In Microsoft Sentinel, the Advanced Security Information Model (ASIM) provides a framework for normalization, defining schemas for various event types like authentication, network sessions, and DNS activity.</p>"},{"location":"parsing/#why-normalize-data","title":"Why Normalize Data?","text":"<p>Data normalization offers several benefits:</p> <p>Consistency: Standardized data fields make data easier to understand. Cross-Source Analysis: You can query and analyze data across multiple sources with a single query. Reduced Complexity: Normalized data means you don\u2019t need to create separate queries for each data source.</p> <p>Parsing and Normalization Example Let\u2019s walk through an example of parsing and normalizing an authentication event.</p>"},{"location":"parsing/#raw-event-data-from-active-directory","title":"Raw Event Data from Active Directory","text":"<p>A typical login event from Active Directory might look like this:</p> <p><pre><code>{\n  \"UserName\": \"Pauline_Muller\",\n  \"LogonType\": \"Interactive\",\n  \"Success\": true,\n  \"SourceIPAddress\": \"192.168.1.15\"\n}\n</code></pre> Step 1: Parse the Data Parsing involves mapping each field in the raw data to a structured format using KQL:</p> <p><pre><code>datatable(UserName:string, LogonType:string, Success:bool, SourceIPAddress:string)\n| extend ParsedUsername = UserName\n| extend ParsedSourceIP = SourceIPAddress\n| extend LogonSuccess = iif(Success == true, \"Success\", \"Failure\")\n</code></pre> After parsing, the data looks like this: <pre><code>{\n  \"ParsedUsername\": \"john_doe\",\n  \"ParsedSourceIP\": \"192.168.1.15\",\n  \"LogonSuccess\": \"Success\",\n  \"LogonType\": \"Interactive\"\n}\n</code></pre> Step 2: Normalize the Data Using ASIM Using ASIM\u2019s Authentication Event Schema, we can map the parsed data to ASIM's standardized fields:</p> <p><pre><code>{\n  \"ActorUserName\": \"john_doe\",\n  \"LogonType\": \"Interactive\",\n  \"LogonResult\": \"Success\",\n  \"SrcIpAddr\": \"192.168.1.15\"\n}\n</code></pre> Now, whether this data is from Active Directory, Okta, or any other source, it will follow the same schema, enabling consistent querying across all sources.</p>"},{"location":"regex/","title":"Why use Regex in parsing?","text":"<p>Regular expressions, or regex, are powerful tools used in parsing to search, match, and extract specific patterns from text data. When working with unstructured or semi-structured data sources, regex helps to identify and isolate relevant information, making it a key component in data parsing.</p>"},{"location":"regex/#reasons-to-use-regex-in-parsing","title":"Reasons to Use Regex in Parsing","text":"<ul> <li> <p>Flexible pattern matching: Regex allows for flexible matching of complex text patterns, making it ideal for handling diverse data formats. For example, a regex pattern can be used to extract IP addresses, email addresses, or specific log entries from large datasets.</p> </li> <li> <p>Extracting key information: Regex helps extract relevant pieces of information from raw data. In logs, for instance, regex can isolate specific fields (such as timestamps, user IDs, and error messages), enabling structured data analysis without manually processing each entry.</p> </li> <li> <p>Efficiency in data transformation: Using regex simplifies the process of transforming unstructured data into a structured format. Instead of manually parsing each line, regex can capture required patterns, helping create structured, consistent data fields from raw text.</p> </li> <li> <p>Consistency across different data sources: Regex is useful for parsing data from various sources that may use different formats. By defining a standard pattern, regex can match fields across diverse formats, aiding in normalization and consistent data analysis.</p> </li> </ul>"},{"location":"regex/#example-of-regex-in-parsing","title":"Example of Regex in parsing","text":"<p>Consider the following raw log entry:</p> <p><pre><code>2023-11-01 12:34:56, INFO, User john_doe logged in from IP 192.168.1.10\n</code></pre> Using regex, we can define patterns to extract specific elements:</p> <ul> <li>Timestamp: <code>plaintext \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}</code></li> <li>Username: <code>plaintext User (\\w+)</code></li> <li>IP Address: <code>plaintext IP (\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})</code></li> </ul> <p>Applying these patterns enables the extraction of timestamp, username, and IP address fields, allowing for structured data analysis.</p>"}]}